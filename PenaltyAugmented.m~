function [xMin, fMin, nIter, info] = PenaltyAugmented(Q, mu0,x0, type, optimizer, varargin)
    % Initialisation
    alpha0 = 1;
    maxIter = 100;
    alpha_max = alpha0;
    tol = 1e-8;
    
    tau = 1e-6;
    epsilon = 1e-6;
  
    if strcmp(type, 'AugmentedLagrangian')
        v_k = varargin{1};
        c_1 = varargin{2};
        info.vks = v_k;
    end
    
    
    % Steepest descent line search strong WC
    lsOptsSteep.c1 = 1e-4;
    lsOptsSteep.c2 = 0.9;
    
    % Initialization
    nIter = 0;
    x_k = x0;
    mu_k = mu0;
    info.xs = x0;
    info.mus = mu0;
    stopCond = false; 
    
    
    while(~stopCond && nIter <= maxIter)
        nIter = nIter + 1;
        switch type
            case 'QuadraticPenalty'
                cur_Q.f = @(x) Q.f(x, mu_k);
                cur_Q.df = @(x) Q.df(x, mu_k);
                switch optimizer
                    case 'SR1'
                        % Trust region parameters 
                        eta = 0.1;  % Step acceptance relative progress threshold
                        Delta = 1; % Trust region radius
                        debug = 0; % Debugging parameter will switch on step by step visualisation of quadratic model and various step options
                        
                        %lsFun = @(x_k, p_k, alpha0) lineSearch(cur_Q, x_k, p_k, alpha_max, lsOptsSteep);
                        $[cur_x, cur_f, nIterSteep, infoSteep] = descentLineSearch(cur_Q, 'steepest', lsFun, alpha0, x_k, tol, maxIter, tau);
                        % Minimisation with 2d subspace and dogleg trust region methods
                        %Fsr1 = cur_Q;
                        %[cur_x, cur_f, nIterSteep, infoSteep] = trustRegion(Fsr1, x_k, @solverCM2dSubspaceExt, Delta, eta, tol, maxIter, tau, debug);
                    case 'ConjugateGrad'
                        
                        maxIter = 100;

                        lsOptsCG_LS.c1 = 1e-4;
                        lsOptsCG_LS.c2 = 0.1;
                        lsFun = @(x_k, p_k, alpha0) lineSearch(cur_Q, x_k, p_k, alpha0, lsOptsCG_LS);
                        [cur_x, cur_f, nIterCG_FR_LS, infoCG_FR_LS] = nonlinearConjugateGradient(cur_Q, lsFun, 'FR', alpha0, x0, tol, maxIter,tau);
                end
                mu_k = mu_k*1.5;
            case 'AugmentedLagrangian'
                cur_Q.f = @(x) Q.f(x, mu_k, v_k);
                cur_Q.df = @(x) Q.df(x, mu_k, v_k);
                %cur_Q.d2f = @(x) Q.d2f(x, mu_k, v_k);
                %lsFun = @(x_k, p_k, alpha0) lineSearch(cur_Q, x_k, p_k, alpha_max, lsOptsSteep);
                %[xSteep, fSteep, nIterSteep, infoSteep] = descentLineSearch(cur_Q, 'bfgs', lsFun, alpha0, x_k, tol, maxIter, tau);
                
                eta = 0.1;  % Step acceptance relative progress threshold
                Delta = 1; % Trust region radius
                debug = 0; % Debugging parameter will switch on step by step visualisation of quadratic model and various step options

                % Minimisation with 2d subspace and dogleg trust region methods
                
                Fsr1 = cur_Q;
                [cur_x, fSteep, nIterSteep, infoSteep] = trustRegion(Fsr1, x_k, @solverCM2dSubspaceExt, Delta, eta, tol, maxIter, tau, debug);
                
                tau = norm(c_1(x_k));
                v_k = v_k - mu_k*c_1(xSteep);
                mu_k = mu_k*1.5;
                info.vks = [info.vks v_k];
        end
        
        if norm(cur_x-x_k, 2) <= epsilon
            stopCond = true; 
        end
        

        
        x_k = cur_x;
        info.mus = [info.mus, mu_k];
        info.xs = [info.xs, x_k];
    end
    
    xMin = cur_x;
    fMin = cur_f;
    q = norm(cur_Q.df(cur_x), 2);
    
end